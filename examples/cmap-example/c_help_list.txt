  -h, --help            show this help message and exit\n");
  -i, --interactive     run in interactive mode\n");
  --interactive-first   run in interactive mode and wait for input right away\n");
  -ins, --instruct      run in instruction mode (use with Alpaca models)\n");
  --multiline-input     allows you to write or paste multiple lines without ending each in '\\'\n");
  -r PROMPT, --reverse-prompt PROMPT\n");
                        halt generation at PROMPT, return control in interactive mode\n");
                        (can be specified more than once for multiple prompts).\n");
  --color               colorise output to distinguish prompt and user input from generations\n");
  -s SEED, --seed SEED  RNG seed (default: -1, use random seed for < 0)\n");
  -t N, --threads N     number of threads to use during generation (default: %d)\n", params.n_threads);
  -tb N, --threads-batch N\n");
                        number of threads to use during batch and prompt processing (default: same as --threads)\n");
  -p PROMPT, --prompt PROMPT\n");
                        prompt to start generation with (default: empty)\n");
  -e, --escape          process prompt escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\)\n");
  --prompt-cache FNAME  file to cache prompt state for faster startup (default: none)\n");
  --prompt-cache-all    if specified, saves user input and generations to cache as well.\n");
                        not supported with --interactive or other interactive options\n");
  --prompt-cache-ro     if specified, uses the prompt cache but does not update it.\n");
  --random-prompt       start with a randomized prompt.\n");
  --in-prefix-bos       prefix BOS to user inputs, preceding the `--in-prefix` string\n");
  --in-prefix STRING    string to prefix user inputs with (default: empty)\n");
  --in-suffix STRING    string to suffix after user inputs with (default: empty)\n");
  -f FNAME, --file FNAME\n");
                        prompt file to start generation.\n");
  -n N, --n-predict N   number of tokens to predict (default: %d, -1 = infinity, -2 = until context filled)\n", params.n_predict);
  -c N, --ctx-size N    size of the prompt context (default: %d, 0 = loaded from model)\n", params.n_ctx);
  -b N, --batch-size N  batch size for prompt processing (default: %d)\n", params.n_batch);
  --top-k N             top-k sampling (default: %d, 0 = disabled)\n", sparams.top_k);
  --top-p N             top-p sampling (default: %.1f, 1.0 = disabled)\n", (double)sparams.top_p);
  --min-p N             min-p sampling (default: %.1f, 0.0 = disabled)\n", (double)sparams.min_p);
  --tfs N               tail free sampling, parameter z (default: %.1f, 1.0 = disabled)\n", (double)sparams.tfs_z);
  --typical N           locally typical sampling, parameter p (default: %.1f, 1.0 = disabled)\n", (double)sparams.typical_p);
  --repeat-last-n N     last n tokens to consider for penalize (default: %d, 0 = disabled, -1 = ctx_size)\n", sparams.penalty_last_n);
  --repeat-penalty N    penalize repeat sequence of tokens (default: %.1f, 1.0 = disabled)\n", (double)sparams.penalty_repeat);
  --presence-penalty N  repeat alpha presence penalty (default: %.1f, 0.0 = disabled)\n", (double)sparams.penalty_present);
  --frequency-penalty N repeat alpha frequency penalty (default: %.1f, 0.0 = disabled)\n", (double)sparams.penalty_freq);
  --mirostat N          use Mirostat sampling.\n");
                        Top K, Nucleus, Tail Free and Locally Typical samplers are ignored if used.\n");
                        (default: %d, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n", sparams.mirostat);
  --mirostat-lr N       Mirostat learning rate, parameter eta (default: %.1f)\n", (double)sparams.mirostat_eta);
  --mirostat-ent N      Mirostat target entropy, parameter tau (default: %.1f)\n", (double)sparams.mirostat_tau);
  -l TOKEN_ID(+/-)BIAS, --logit-bias TOKEN_ID(+/-)BIAS\n");
                        modifies the likelihood of token appearing in the completion,\n");
                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n");
                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n");
  --grammar GRAMMAR     BNF-like grammar to constrain generations (see samples in grammars/ dir)\n");
  --grammar-file FNAME  file to read grammar from\n");
  --cfg-negative-prompt PROMPT\n");
                        negative prompt to use for guidance. (default: empty)\n");
  --cfg-negative-prompt-file FNAME\n");
                        negative prompt file to use for guidance. (default: empty)\n");
  --cfg-scale N         strength of guidance (default: %f, 1.0 = disable)\n", sparams.cfg_scale);
  --rope-scaling {none,linear,yarn}\n");
                        RoPE frequency scaling method, defaults to linear unless specified by the model\n");
  --rope-scale N        RoPE context scaling factor, expands context by a factor of N\n");
  --rope-freq-base N    RoPE base frequency, used by NTK-aware scaling (default: loaded from model)\n");
  --rope-freq-scale N   RoPE frequency scaling factor, expands context by a factor of 1/N\n");
  --yarn-orig-ctx N     YaRN: original context size of model (default: 0 = model training context size)\n");
  --yarn-ext-factor N   YaRN: extrapolation mix factor (default: 1.0, 0.0 = full interpolation)\n");
  --yarn-attn-factor N  YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n");
  --yarn-beta-slow N    YaRN: high correction dim or alpha (default: %.1f)\n", params.yarn_beta_slow);
  --yarn-beta-fast N    YaRN: low correction dim or beta (default: %.1f)\n", params.yarn_beta_fast);
  --ignore-eos          ignore end of stream token and continue generating (implies --logit-bias 2-inf)\n");
  --no-penalize-nl      do not penalize newline token\n");
  --memory-f32          use f32 instead of f16 for memory key+value (default: disabled)\n");
                        not recommended: doubles context memory required and no measurable increase in quality\n");
  --temp N              temperature (default: %.1f)\n", (double)sparams.temp);
  --logits-all          return logits for all tokens in the batch (default: disabled)\n");
  --hellaswag           compute HellaSwag score over random tasks from datafile supplied with -f\n");
  --hellaswag-tasks N   number of tasks to use when computing the HellaSwag score (default: %zu)\n", params.hellaswag_tasks);
  --keep N              number of tokens to keep from the initial prompt (default: %d, -1 = all)\n", params.n_keep);
  --draft N             number of tokens to draft for speculative decoding (default: %d)\n", params.n_draft);
  --chunks N            max number of chunks to process (default: %d, -1 = all)\n", params.n_chunks);
  -np N, --parallel N   number of parallel sequences to decode (default: %d)\n", params.n_parallel);
  -ns N, --sequences N  number of sequences to decode (default: %d)\n", params.n_sequences);
  -pa N, --p-accept N   speculative decoding accept probability (default: %.1f)\n", (double)params.p_accept);
  -ps N, --p-split N    speculative decoding split probability (default: %.1f)\n", (double)params.p_split);
  -cb, --cont-batching  enable continuous batching (a.k.a dynamic batching) (default: disabled)\n");
  --mmproj MMPROJ_FILE  path to a multimodal projector file for LLaVA. see examples/llava/README.md\n");
  --image IMAGE_FILE    path to an image file. use with multimodal models\n");
  --mlock               force system to keep model in RAM rather than swapping or compressing\n");
  --no-mmap             do not memory-map model (slower load but may reduce pageouts if not using mlock)\n");
  --numa                attempt optimizations that help on some NUMA systems\n");
                        if run without this previously, it is recommended to drop the system page cache before using this\n");
                        see https://github.com/ggerganov/llama.cpp/issues/1437\n");
  -ngl N, --n-gpu-layers N\n");
                        number of layers to store in VRAM\n");
  -ngld N, --n-gpu-layers-draft N\n");
                        number of layers to store in VRAM for the draft model\n");
  -ts SPLIT --tensor-split SPLIT\n");
                        how to split tensors across multiple GPUs, comma-separated list of proportions, e.g. 3,1\n");
  -mg i, --main-gpu i   the GPU to use for scratch and small tensors\n");
  -nommq, --no-mul-mat-q\n");
                        use " GGML_CUBLAS_NAME " instead of custom mul_mat_q " GGML_CUDA_NAME " kernels.\n");
                        Not recommended since this is both slower and uses more VRAM.\n");
  --verbose-prompt      print prompt before generation\n");
  --simple-io           use basic IO for better compatibility in subprocesses and limited consoles\n");
  --lora FNAME          apply LoRA adapter (implies --no-mmap)\n");
  --lora-scaled FNAME S apply LoRA adapter with user defined scaling S (implies --no-mmap)\n");
  --lora-base FNAME     optional model to use as a base for the layers modified by the LoRA adapter\n");
  -m FNAME, --model FNAME\n");
                        model path (default: %s)\n", params.model.c_str());
  -md FNAME, --model-draft FNAME\n");
                        draft model for speculative decoding (default: %s)\n", params.model.c_str());
  -ld LOGDIR, --logdir LOGDIR\n");
                        path under which to save YAML logs (no logging if unset)\n");
\n");
tderr, "%s: error: failed to load model '%s'\n", __func__, params.model.c_str());
tderr, "%s: error: failed to create context with model '%s'\n", __func__, params.model.c_str());
tderr, "%s: error: failed to apply lora adapter\n", __func__);
tream, "%s:\n", prop_name);
tream, "%s: [", prop_name);
tream, "%e, ", data[i]);
tream, "%e]\n", data.back());
tream, "%s:\n", prop_name);
tream, "%s: [", prop_name);
tream, "%d, ", data[i]);
tream, "%d]\n", data.back());
tream, "%s:\n", prop_name);
tream, "%s: %s\n", prop_name, data_str.c_str());
tream, "%s: %s\n", prop_name, data_str.c_str());
tream, "%s: |\n", prop_name);
tream, "  %s\n", data_str.substr(pos_start, pos_found-pos_start).c_str());
imestamp_ns, 11, "%09" PRId64, ns);
tream, "build_commit: %s\n",        LLAMA_COMMIT);
tream, "build_number: %d\n",        LLAMA_BUILD_NUMBER);
tream, "cpu_has_arm_fma: %s\n",     ggml_cpu_has_arm_fma()     ? "true" : "false");
tream, "cpu_has_avx: %s\n",         ggml_cpu_has_avx()         ? "true" : "false");
tream, "cpu_has_avx2: %s\n",        ggml_cpu_has_avx2()        ? "true" : "false");
tream, "cpu_has_avx512: %s\n",      ggml_cpu_has_avx512()      ? "true" : "false");
tream, "cpu_has_avx512_vbmi: %s\n", ggml_cpu_has_avx512_vbmi() ? "true" : "false");
tream, "cpu_has_avx512_vnni: %s\n", ggml_cpu_has_avx512_vnni() ? "true" : "false");
tream, "cpu_has_blas: %s\n",        ggml_cpu_has_blas()        ? "true" : "false");
tream, "cpu_has_cublas: %s\n",      ggml_cpu_has_cublas()      ? "true" : "false");
tream, "cpu_has_clblast: %s\n",     ggml_cpu_has_clblast()     ? "true" : "false");
tream, "cpu_has_fma: %s\n",         ggml_cpu_has_fma()         ? "true" : "false");
tream, "cpu_has_gpublas: %s\n",     ggml_cpu_has_gpublas()     ? "true" : "false");
tream, "cpu_has_neon: %s\n",        ggml_cpu_has_neon()        ? "true" : "false");
tream, "cpu_has_f16c: %s\n",        ggml_cpu_has_f16c()        ? "true" : "false");
tream, "cpu_has_fp16_va: %s\n",     ggml_cpu_has_fp16_va()     ? "true" : "false");
tream, "cpu_has_wasm_simd: %s\n",   ggml_cpu_has_wasm_simd()   ? "true" : "false");
tream, "cpu_has_blas: %s\n",        ggml_cpu_has_blas()        ? "true" : "false");
tream, "cpu_has_sse3: %s\n",        ggml_cpu_has_sse3()        ? "true" : "false");
tream, "cpu_has_vsx: %s\n",         ggml_cpu_has_vsx()         ? "true" : "false");
tream, "debug: false\n");
tream, "debug: true\n");
tream, "model_desc: %s\n", model_desc);
tream, "n_vocab: %d  # output size of the final layer, 32001 for some models\n", llama_n_vocab(llama_get_model(lctx)));
tream, "optimize: true\n");
tream, "optimize: false\n");
tream, "time: %s\n", timestamp.c_str());
tream, "\n");
tream, "###############\n");
tream, "# User Inputs #\n");
tream, "###############\n");
tream, "\n");
tream, "alias: %s # default: unknown\n", params.model_alias.c_str());
tream, "batch_size: %d # default: 512\n", params.n_batch);
tream, "cfg_scale: %f # default: 1.0\n", sparams.cfg_scale);
tream, "chunks: %d # default: -1 (unlimited)\n", params.n_chunks);
tream, "color: %s # default: false\n", params.use_color ? "true" : "false");
tream, "ctx_size: %d # default: 512\n", params.n_ctx);
tream, "escape: %s # default: false\n", params.escape ? "true" : "false");
tream, "file: # never logged, see prompt instead. Can still be specified for input.\n");
tream, "frequency_penalty: %f # default: 0.0 \n", sparams.penalty_freq);
tream, "grammar-file: # never logged, see grammar instead. Can still be specified for input.\n");
tream, "hellaswag: %s # default: false\n", params.hellaswag ? "true" : "false");
tream, "hellaswag_tasks: %zu # default: 400\n", params.hellaswag_tasks);
tream, "ignore_eos: %s # default: false\n", ignore_eos ? "true" : "false");
tream, "in_prefix_bos: %s # default: false\n", params.input_prefix_bos ? "true" : "false");
tream, "instruct: %s # default: false\n", params.instruct ? "true" : "false");
tream, "interactive: %s # default: false\n", params.interactive ? "true" : "false");
tream, "interactive_first: %s # default: false\n", params.interactive_first ? "true" : "false");
tream, "keep: %d # default: 0\n", params.n_keep);
tream, "logdir: %s # default: unset (no logging)\n", params.logdir.c_str());
tream, "logit_bias:\n");
tream, "  %d: %f", lb.first, lb.second);
tream, "lora:\n");
tream, "  - %s\n", std::get<0>(la).c_str());
tream, "lora_scaled:\n");
tream, "  - %s: %f\n", std::get<0>(la).c_str(), std::get<1>(la));
tream, "lora_base: %s\n", params.lora_base.c_str());
tream, "main_gpu: %d # default: 0\n", params.main_gpu);
tream, "memory_f32: %s # default: false\n", !params.memory_f16 ? "true" : "false");
tream, "mirostat: %d # default: 0 (disabled)\n", sparams.mirostat);
tream, "mirostat_ent: %f # default: 5.0\n", sparams.mirostat_tau);
tream, "mirostat_lr: %f # default: 0.1\n", sparams.mirostat_eta);
tream, "mlock: %s # default: false\n", params.use_mlock ? "true" : "false");
tream, "model: %s # default: models/7B/ggml-model.bin\n", params.model.c_str());
tream, "model_draft: %s # default:\n", params.model_draft.c_str());
tream, "multiline_input: %s # default: false\n", params.multiline_input ? "true" : "false");
tream, "n_gpu_layers: %d # default: -1\n", params.n_gpu_layers);
tream, "n_predict: %d # default: -1 (unlimited)\n", params.n_predict);
tream, "n_probs: %d # only used by server binary, default: 0\n", sparams.n_probs);
tream, "no_mmap: %s # default: false\n", !params.use_mmap ? "true" : "false");
tream, "no_mul_mat_q: %s # default: false\n", !params.mul_mat_q ? "true" : "false");
tream, "no_penalize_nl: %s # default: false\n", !sparams.penalize_nl ? "true" : "false");
tream, "numa: %s # default: false\n", params.numa ? "true" : "false");
tream, "ppl_output_type: %d # default: 0\n", params.ppl_output_type);
tream, "ppl_stride: %d # default: 0\n", params.ppl_stride);
tream, "presence_penalty: %f # default: 0.0\n", sparams.penalty_present);
tream, "prompt_cache: %s\n", params.path_prompt_cache.c_str());
tream, "prompt_cache_all: %s # default: false\n", params.prompt_cache_all ? "true" : "false");
tream, "prompt_cache_ro: %s # default: false\n", params.prompt_cache_ro ? "true" : "false");
tream, "random_prompt: %s # default: false\n", params.random_prompt ? "true" : "false");
tream, "repeat_penalty: %f # default: 1.1\n", sparams.penalty_repeat);
tream, "reverse_prompt:\n");
tream, "  - %s\n", ap.c_str());
tream, "rope_freq_base: %f # default: 10000.0\n", params.rope_freq_base);
tream, "rope_freq_scale: %f # default: 1.0\n", params.rope_freq_scale);
tream, "seed: %d # default: -1 (random seed)\n", params.seed);
tream, "simple_io: %s # default: false\n", params.simple_io ? "true" : "false");
tream, "cont_batching: %s # default: false\n", params.cont_batching ? "true" : "false");
tream, "temp: %f # default: 0.8\n", sparams.temp);
tream, "tfs: %f # default: 1.0\n", sparams.tfs_z);
tream, "threads: %d # default: %d\n", params.n_threads, std::thread::hardware_concurrency());
tream, "top_k: %d # default: 40\n", sparams.top_k);
tream, "top_p: %f # default: 0.95\n", sparams.top_p);
tream, "min_p: %f # default: 0.0\n", sparams.min_p);
tream, "typical_p: %f # default: 1.0\n", sparams.typical_p);
tream, "verbose_prompt: %s # default: false\n", params.verbose_prompt ? "true" : "false");
