Content: usage: %s [options]\n
Content: \n
Content: options:\n
Content:   _h, __help            show this help message and exit\n
Content:   _i, __interactive     run in interactive mode\n
Content:   __interactive_first   run in interactive mode and wait for input right away\n
Content:   _ins, __instruct      run in instruction mode (use with Alpaca models)\n
Content:   __multiline_input     allows you to write or paste multiple lines without ending each in '\\'\n
Content:   _r PROMPT, __reverse_prompt PROMPT\n
Content:                         halt generation at PROMPT, return control in interactive mode\n
Content:                         (can be specified more than once for multiple prompts).\n
Content:   __color               colorise output to distinguish prompt and user input from generations\n
Content:   _s SEED, __seed SEED  RNG seed (default: _1, use random seed for < 0)\n
Content:   _t N, __threads N     number of threads to use during generation (default: %d)\n
Content:   _tb N, __threads_batch N\n
Content:                         number of threads to use during batch and prompt processing (default: same as __threads)\n
Content:   _p PROMPT, __prompt PROMPT\n
Content:                         prompt to start generation with (default: empty)\n
Content:   _e, __escape          process prompt escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\)\n
Content:   __prompt_cache FNAME  file to cache prompt state for faster startup (default: none)\n
Content:   __prompt_cache_all    if specified, saves user input and generations to cache as well.\n
Content:                         not supported with __interactive or other interactive options\n
Content:   __prompt_cache_ro     if specified, uses the prompt cache but does not update it.\n
Content:   __random_prompt       start with a randomized prompt.\n
Content:   __in_prefix_bos       prefix BOS to user inputs, preceding the `__in_prefix` string\n
Content:   __in_prefix STRING    string to prefix user inputs with (default: empty)\n
Content:   __in_suffix STRING    string to suffix after user inputs with (default: empty)\n
Content:   _f FNAME, __file FNAME\n
Content:                         prompt file to start generation.\n
Content:   _n N, __n_predict N   number of tokens to predict (default: %d, _1 = infinity, _2 = until context filled)\n
Content:   _c N, __ctx_size N    size of the prompt context (default: %d, 0 = loaded from model)\n
Content:   _b N, __batch_size N  batch size for prompt processing (default: %d)\n
Content:   __top_k N             top_k sampling (default: %d, 0 = disabled)\n
Content:   __top_p N             top_p sampling (default: %.1f, 1.0 = disabled)\n
Content:   __tfs N               tail free sampling, parameter z (default: %.1f, 1.0 = disabled)\n
Content:   __typical N           locally typical sampling, parameter p (default: %.1f, 1.0 = disabled)\n
Content:   __repeat_last_n N     last n tokens to consider for penalize (default: %d, 0 = disabled, _1 = ctx_size)\n
Content:   __repeat_penalty N    penalize repeat sequence of tokens (default: %.1f, 1.0 = disabled)\n
Content:   __presence_penalty N  repeat alpha presence penalty (default: %.1f, 0.0 = disabled)\n
Content:   __frequency_penalty N repeat alpha frequency penalty (default: %.1f, 0.0 = disabled)\n
Content:   __mirostat N          use Mirostat sampling.\n
Content:                         Top K, Nucleus, Tail Free and Locally Typical samplers are ignored if used.\n
Content:                         (default: %d, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n
Content:   __mirostat_lr N       Mirostat learning rate, parameter eta (default: %.1f)\n
Content:   __mirostat_ent N      Mirostat target entropy, parameter tau (default: %.1f)\n
Content:   _l T, __logit_bias T  T = TOKEN_ID(plus/minus)BIAS\n
Content:                         modifies the likelihood of token appearing in the completion,\n
Content:                         i.e. `__logit_bias 15043+1` to increase likelihood of token ' Hello',\n
Content:                         or `__logit_bias 15043_1` to decrease likelihood of token ' Hello'\n
Content:   __grammar GRAMMAR     BNF_like grammar to constrain generations (see samples in grammars/ dir)\n
Content:   __grammar_file FNAME  file to read grammar from\n
Content:   __cfg_negative_prompt PROMPT\n
Content:                         negative prompt to use for guidance. (default: empty)\n
Content:   __cfg_negative_prompt_file FNAME\n
Content:                         negative prompt file to use for guidance. (default: empty)\n
Content:   __cfg_scale N         strength of guidance (default: %f, 1.0 = disable)\n
Content:   __rope_scale N        RoPE context linear scaling factor, inverse of __rope_freq_scale\n
Content:   __rope_freq_base N    RoPE base frequency, used by NTK_aware scaling (default: loaded from model)\n
Content:   __rope_freq_scale N   RoPE frequency linear scaling factor (default: loaded from model)\n
Content:   __ignore_eos          ignore end of stream token and continue generating (implies __logit_bias 2_inf)\n
Content:   __no_penalize_nl      do not penalize newline token (default is DO penalise nl token)\n
Content:   __memory_f32          use f32 instead of f16 for memory key+value (default: disabled)\n
Content:                         not recommended: doubles context memory required and no measurable increase in quality\n
Content:   __temp N              temperature (default: %.1f)\n
Content:   __logits_all          return logits for all tokens in the batch (default: disabled)\n
Content:   __hellaswag           compute HellaSwag score over random tasks from datafile supplied with _f\n
Content:   __hellaswag_tasks N   number of tasks to use when computing the HellaSwag score (default: %zu)\n
Content:   __keep N              number of tokens to keep from the initial prompt (default: %d, _1 = all)\n
Content:   __draft N             number of tokens to draft for speculative decoding (default: %d)\n
Content:   __chunks N            max number of chunks to process (default: %d, _1 = all)\n
Content:   _np N, __parallel N   number of parallel sequences to decode (default: %d)\n
Content:   _ns N, __sequences N  number of sequences to decode (default: %d)\n
Content:   _cb, __cont_batching  enable continuous batching (a.k.a dynamic batching) (default: disabled)\n
Content:   __mlock               force system to keep model in RAM rather than swapping or compressing\n
Content:   __no_mmap             do not memory_map model (slower load but may reduce pageouts if not using mlock)\n
Content:   __numa                attempt optimizations that help on some NUMA systems\n
Content:                         if run without this previously, it is recommended to drop the system page cache before using this\n
Content:                         see https://github.com/ggerganov/llama.cpp/issues/1437\n
Content:   _ngl N, __n_gpu_layers N\n
Content:                         number of layers to store in VRAM\n
Content:   _ngld N, __n_gpu_layers_draft N\n
Content:                         number of layers to store in VRAM for the draft model\n
Content:   _ts SPLIT __tensor_split SPLIT\n
Content:                         how to split tensors across multiple GPUs, comma_separated list of proportions, e.g. 3,1\n
Content:   _mg i, __main_gpu i   the GPU to use for scratch and small tensors\n
Content:   _nommq, __no_mul_mat_q\n
Content:                         use " GGML_CUBLAS_NAME " instead of custom mul_mat_q " GGML_CUDA_NAME " kernels.\n
Content:                         Not recommended since this is both slower and uses more VRAM.\n
Content:   __verbose_prompt      print prompt before generation\n
Content:   __lora FNAME          apply LoRA adapter (implies __no_mmap)\n
Content:   __lora_scaled FNAME S apply LoRA adapter with user defined scaling S (implies __no_mmap)\n
Content:   __lora_base FNAME     optional model to use as a base for the layers modified by the LoRA adapter\n
Content:   _m FNAME, __model FNAME\n
Content:                         model path (default: %s)\n
Content:   _md FNAME, __model_draft FNAME\n
Content:                         draft model for speculative decoding (default: %s)\n
Content:   _ld LOGDIR, __logdir LOGDIR\n
Content:                         path under which to save YAML logs (no logging if unset)\n
Content:   __ppl_stride          stride for ppl calcs. 0 (default): the pre_existing approach will be used.\n
Content:   __ppl_output_type     0 (default): ppl output as usual, 1: ppl output num_tokens, one per line\n
Content:   __embedding           0 (default): get only sentence embedding\n
Content:   __beams N             0 (default): if non_zero use beam search of given width N.\n
Content:   __memory_f32          0 (default): if true (= 1) disable f16 memory.\n
Content:   __no_mmap             0 (default): if true use mmap for faster loads.\n
Content:   __mlock               0 (default): if true keep model in memory.\n
Content:   __use_color           0 (default): use color to distinguish generations from inputs\n
Content:   __nprobs N            if > 0 output the probabilities of the top N tokens\n
Content:   __alias               model alias (default: 'unknown')\n
Content:   __infill              0 (defaut) use infill mode\n
Content:   __prompt_file         name of external prompt file\n
Content: \n
